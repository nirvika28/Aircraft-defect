# -*- coding: utf-8 -*-
"""AerospaceMaterial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BjyReRMMtF1Y_FIKYGBpglNF_GcATMUA
"""

from google.colab import drive
import os

drive.mount('/content/drive')

!pip install ultralytics

from ultralytics import YOLO
import os

import zipfile

zip_path = '/content/aircraft-skin-defects.v4-classification-isolated-5-classes-grayscale-prep.yolov5pytorch.zip'
extract_path = '/content/aircraft_yolo'


!unzip -q "$zip_path" -d "$extract_path"

data_yaml = "/content/aircraft_yolo/data.yaml"
print(open(data_yaml).read())

import os
import glob
from collections import Counter
import matplotlib.pyplot as plt

def count_yolo_classes(labels_path, class_names=None):
    """
    labels_path: path/to/train/labels or valid/labels
    class_names: list of class names from YAML file
    """
    class_counts = Counter()

    # Get all .txt label files
    label_files = glob.glob(os.path.join(labels_path, "*.txt"))

    for lf in label_files:
        with open(lf, "r") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) > 0:
                    class_id = int(parts[0])
                    class_counts[class_id] += 1

    # Print counts
    print("\nüìå Class Counts for:", labels_path)
    for cid, count in class_counts.items():
        name = class_names[cid] if class_names else f"Class {cid}"
        print(f" - {name}: {count}")

    # Plot distribution
    if class_counts:
        plt.figure(figsize=(5,4))
        labels = [class_names[c] if class_names else c for c in class_counts.keys()]
        values = list(class_counts.values())
        plt.bar(labels, values)
        plt.title(f"Class Distribution: {labels_path}")
        plt.xlabel("Classes")
        plt.ylabel("Number of Annotations")
        plt.show()

    return class_counts


# ===============================
# 1Ô∏è‚É£ Load class names from data.yaml
# ===============================

import yaml

yaml_path = "/content/aircraft_yolo/data.yaml"  # CHANGE IF NEEDED

with open(yaml_path, 'r') as f:
    data_yaml = yaml.safe_load(f)

class_names = data_yaml["names"]
print("\nüü¶ Class Names:", class_names)


# ===============================
# 2Ô∏è‚É£ Check Train / Valid / Test
# ===============================

train_counts = count_yolo_classes("/content/aircraft_yolo/train/labels", class_names)
valid_counts = count_yolo_classes("/content/aircraft_yolo/valid/labels", class_names)
test_counts  = count_yolo_classes("/content/aircraft_yolo/test/labels", class_names)

data_root = "/content/aircraft_yolo"
train_img = f"{data_root}/train/images"
train_lbl = f"{data_root}/train/labels"

valid_img = f"{data_root}/valid/images"
valid_lbl = f"{data_root}/valid/labels"

test_img = f"{data_root}/test/images"
test_lbl = f"{data_root}/test/labels"

from collections import Counter
import glob

def count_classes(lbl_path):
    cnt = Counter()
    for lbl in glob.glob(lbl_path + "/*.txt"):
        for line in open(lbl):
            cid = int(line.split()[0])
            cnt[cid] += 1
    return cnt

print("TRAIN:", count_classes(train_lbl))
print("VALID:", count_classes(valid_lbl))
print("TEST:",  count_classes(test_lbl))

import os
import shutil
from ultralytics import YOLO
from tqdm import tqdm

"""Oversampling the classes, as there is imbalance among features"""

import random

def oversample_class(class_id, target_count):
    all_labels = glob.glob(train_lbl + "/*.txt")
    files = []

    # Find all images for this class
    for f in all_labels:
        for line in open(f):
            cid = int(line.split()[0])
            if cid == class_id:
                files.append(f)
                break

    orig = len(files)
    needed = target_count - orig

    print(f"Oversampling class {class_id}: original={orig}, target={target_count}, adding {needed}")

    for i in range(needed):
        lbl_src = random.choice(files)
        img_src = lbl_src.replace("/labels/", "/images/").replace(".txt", ".jpg")

        # Copy with new name
        lbl_dst = lbl_src.replace(".txt", f"_aug{i}.txt")
        img_dst = img_src.replace(".jpg", f"_aug{i}.jpg")

        shutil.copy(img_src, img_dst)
        shutil.copy(lbl_src, lbl_dst)

# Based on your counts:
# dent = 1346 ‚Üí target for all classes
target = 1346

# your classes = ['crack', 'dent', 'missing-head', 'paint-off', 'scratch']
oversample_class(0, target)  # crack
oversample_class(2, target)  # missing-head
oversample_class(3, target)  # paint-off
oversample_class(4, target)  # scratch

yaml_path = f"{data_root}/data.yaml"

yaml_new = """train: train/images
val: valid/images
test: test/images

nc: 5
names: ["crack", "dent", "missing-head", "paint-off", "scratch"]

# class weights to fix imbalance
weights: [1.3, 1.0, 1.2, 1.5, 2.5]   # crack, dent, missing-head, paint-off, scratch
"""

with open(yaml_path, "w") as f:
    f.write(yaml_new)

print("Updated YAML:")
print(open(yaml_path).read())

"""LOAD yolo model"""

model = YOLO("yolo11s.pt")

model.train(
    data=yaml_path,
    epochs=25,      # enough when dataset is balanced
    imgsz=768,      # big enough for cracks, fast enough for Colab
    batch=32,
    lr0=0.001,
    mosaic=1.0,
    mixup=0.2,
    hsv_h=0.015,
    hsv_s=0.7,
    hsv_v=0.4,
    translate=0.1,
    scale=0.5,
    shear=0.2,
    flipud=0.2,
    fliplr=0.5,
    cache=True,
    device=0
)

"""Model's accuracy:"""

model.val()

import cv2
import matplotlib.pyplot as plt

def check_defect(image_path):
    results = model.predict(image_path, imgsz=1024, conf=0.25)
    boxes = results[0].boxes
    img = results[0].plot()

    if boxes is None or len(boxes) == 0:
        print("üü¢ NO DEFECT detected")
        label = "No Defect"
    else:
        print("üî¥ DEFECT detected:")
        for b in boxes:
            cid = int(b.cls)
            conf = float(b.conf)
            name = model.names[cid]
            print(f" - {name} ({conf:.2f})")
        label = "Defect(s) Found"

    plt.figure(figsize=(8,8))
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(label)
    plt.axis("off")
    plt.show()

"""Confusion matrix showing model's progress"""

import matplotlib.pyplot as plt
import cv2

cm_path = "/content/runs/detect/val/confusion_matrix.png"
img = cv2.imread(cm_path)
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.axis("off")
plt.show()

"""##Defect type recognition

"""

check_defect("/def.png")
check_defect("/def2.png")

"""Speech integration"""

##STRESS AND SPEECH VARIABILITY

!pip install SpeechRecognition pydub gtts

import speech_recognition as sr
from gtts import gTTS
from pydub import AudioSegment
import os

text = "Autopilot disengaged. Reduce altitude to five thousand feet."

tts = gTTS(text=text, lang='en')
tts.save("normal.mp3")

# Convert MP3 ‚Üí WAV for SpeechRecognition
AudioSegment.from_mp3("normal.mp3").export("normal.wav", format="wav")

audio = AudioSegment.from_mp3("normal.mp3")

# Increase pitch and speed
high_pitch = audio._spawn(audio.raw_data, overrides={
    "frame_rate": int(audio.frame_rate * 1.3)
}).set_frame_rate(audio.frame_rate)

stressed = high_pitch.speedup(playback_speed=1.4)
stressed.export("stressed.wav", format="wav")

!pip install openai-whisper
import whisper
model = whisper.load_model("tiny")
result = model.transcribe("stressed.wav")
print(result["text"])

from pydub.effects import normalize, speedup

# Reload stressed audio
stressed_audio = AudioSegment.from_wav("stressed.wav")

# Normalize loudness & slightly slow down (simulate correction)
fixed = normalize(stressed_audio)
fixed = speedup(fixed, playback_speed=0.8)   # slower = easier for ASR
fixed.export("stressed_fixed.wav", format="wav")

# Re-run recognition
fixed_text = transcribe_audio("stressed_fixed.wav")
print("üü¢ Recovered Speech Recognition:", fixed_text)

recognizer = sr.Recognizer()

def transcribe_audio(path):
    with sr.AudioFile(path) as source:
        audio = recognizer.record(source)
    try:
        return recognizer.recognize_google(audio)
    except Exception as e:
        return f"Error: {str(e)}"

normal_text = transcribe_audio("normal.wav")
stressed_text = transcribe_audio("stressed.wav")

print("üü¢ Normal Speech Recognition:", normal_text)
print("üî¥ Stressed Speech Recognition:", stressed_text)

import matplotlib.pyplot as plt
import numpy as np

# Convert to waveform arrays
import librosa
y1, sr1 = librosa.load("normal.mp3", sr=None)
y2, sr2 = librosa.load("stressed.mp3", sr=None)

plt.figure(figsize=(12,4))
plt.subplot(2,1,1)
plt.plot(y1)
plt.title("Normal Speech Waveform")

plt.subplot(2,1,2)
plt.plot(y2, color='r')
plt.title("Stressed Speech Waveform (High Pitch + Fast)")

plt.tight_layout()
plt.show()

"""SOLVING THIS ISSUE:"""

!pip install noisereduce
import noisereduce as nr
import librosa
import soundfile as sf

y, sr = librosa.load("stressed.wav", sr=None)
reduced = nr.reduce_noise(y=y, sr=sr)
sf.write("cleaned.wav", reduced, sr)

!pip install openai-whisper
import whisper

model = whisper.load_model("base")
result = model.transcribe("stressed.wav")
print("Whisper Transcription:", result["text"])

import whisper
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load Whisper model for speech-to-text
asr_model = whisper.load_model("base")  # can use "small" or "medium" for more accuracy

# Load semantic correction model
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
t5_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

def transcribe_audio_whisper(audio_path):
    print(f"üéß Transcribing: {audio_path}")
    result = asr_model.transcribe(audio_path)
    transcript = result["text"].strip()
    print("üó£Ô∏è Raw Transcription:", transcript)
    return transcript


def correct_semantically(text):
    print("üîß Correcting transcription semantically...")
    prompt = f"Correct this aviation command for meaning and clarity:\n{text}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    outputs = t5_model.generate(**inputs, max_new_tokens=64)
    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return corrected_text

def process_speech_command(audio_path):
    raw_text = transcribe_audio_whisper(audio_path)
    corrected_text = correct_semantically(raw_text)

    print("\nüü¢ Final Corrected Command:")
    print(corrected_text)
    return corrected_text
# Example
audio_path = "/content/stressed.wav"  # <-- replace with your file path
final_output = process_speech_command(audio_path)

# =======================================================
# ‚úÖ Generic Self-Correcting Speech Recognition System
# Author: [Your Name]
# Use Case: Aviation Speech under Stress / Noise
# Models Used: OpenAI Whisper (large) - context aware ASR
# =======================================================

!pip install -q openai-whisper torch numpy

import whisper
import numpy as np

# ------------------------------
# Load Whisper Model (large recommended for context understanding)
# ------------------------------
model = whisper.load_model("large")

# ------------------------------
# Transcription with Auto Re-Decoding
# ------------------------------
def smart_transcribe(audio_path, low_conf_threshold=-0.5):
    """
    Transcribes and auto-corrects low-confidence segments automatically.
    No manual prompts or domain rules used.
    """

    print(f"üéß Transcribing: {audio_path}")

    # First-pass transcription
    result = model.transcribe(
        audio_path,
        temperature=0.0,         # deterministic decoding
        best_of=5,               # multiple candidates
        beam_size=5,             # improves semantic coherence
        no_speech_threshold=0.3  # ignore background noise
    )

    print("üó£Ô∏è Initial Transcription:")
    print(result["text"].strip())

    # ------------------------------
    # Step 2: Auto-detect low-confidence segments
    # ------------------------------
    segments = result.get("segments", [])
    low_conf_segments = [s for s in segments if s.get("avg_logprob", 0) < low_conf_threshold]

    if not low_conf_segments:
        print("‚úÖ No low-confidence regions found.")
        return result["text"].strip()

    print(f"‚ö†Ô∏è Found {len(low_conf_segments)} low-confidence region(s). Re-decoding...")

    # ------------------------------
    # Step 3: Re-decode only uncertain parts with higher temperature (exploration)
    # ------------------------------
    fixed_segments = []
    for seg in segments:
        if seg in low_conf_segments:
            print(f"üîß Re-decoding: '{seg['text']}' (conf={seg['avg_logprob']:.2f})")
            new_result = model.transcribe(
                audio_path,
                temperature=0.6,  # allows creativity to explore alternatives
                initial_prompt=seg["text"]
            )
            fixed_text = new_result["text"].strip()
            print(f"‚û°Ô∏è Fixed segment: {fixed_text}")
            fixed_segments.append(fixed_text)
        else:
            fixed_segments.append(seg["text"].strip())

    # ------------------------------
    # Step 4: Merge segments back together
    # ------------------------------
    corrected_text = " ".join(fixed_segments)
    print("\nüü¢ Final Corrected Transcription:")
    print(corrected_text)
    return corrected_text

!pip install gtts pydub numpy
from gtts import gTTS
from pydub import AudioSegment
from pydub.generators import WhiteNoise, Sine
import numpy as np

# -----------------------------------------------
# Base aviation command
# -----------------------------------------------
text = "Autopilot disengaged. Reduce altitude to five thousand feet."
tts = gTTS(text=text, lang='en')
tts.save("base.mp3")

audio = AudioSegment.from_mp3("base.mp3")

# -----------------------------------------------
# Add cockpit noise (engine rumble)
# -----------------------------------------------
engine_noise = Sine(60).to_audio_segment(duration=len(audio)) - 20  # deep rumble
wind_noise = WhiteNoise().to_audio_segment(duration=len(audio)) - 25

# Layer the noises
noisy = audio.overlay(engine_noise).overlay(wind_noise)

# -----------------------------------------------
# Add radio static (crackle)
# -----------------------------------------------
static = WhiteNoise().to_audio_segment(duration=len(audio)) - 15
noisy = noisy.overlay(static)

# -----------------------------------------------
# Add clipping + distortion
# -----------------------------------------------
distorted = noisy + 10            # amplify
distorted = distorted.set_frame_rate(8000)  # reduce quality (radio effect)
distorted = distorted.low_pass_filter(1500) # muffling
distorted = distorted.high_pass_filter(200)

# -----------------------------------------------
# Random jitter (simulate shaky radio)
# -----------------------------------------------
samples = np.array(distorted.get_array_of_samples())
jitter = samples.astype(np.float32)
jitter *= np.random.uniform(0.8, 1.2, size=jitter.shape)
jitter = jitter.astype(np.int16)

extreme = distorted._spawn(jitter)
extreme.export("extreme_distorted.wav", format="wav")

print("üî• Extreme distorted test file created: extreme_distorted.wav")

!ls /content

audio_path = "/content/extreme_distorted.wav"
final_text = smart_transcribe(audio_path)

"""In my project, I initially implemented a speech recognition pipeline using the Whisper-base model along with a FLAN-T5 semantic correction layer. However, this approach performed poorly for aviation commands spoken under stress. For example, when the input audio said ‚ÄúAutopilot disengaged, reduce altitude to 5000 feet,‚Äù Whisper-base misheard it as ‚ÄúOur pilot is engaged, reduce altitude of 5000 feet,‚Äù completely reversing the meaning. FLAN-T5 also failed to correct this error because it only improves grammar and fluency, not domain-specific semantics. To overcome these limitations, I replaced this setup with a more robust system based on Whisper-large combined with a confidence-based self-correction mechanism. Whisper-large correctly understood the stressed and even heavily distorted audio inputs without any manual prompting or rule-based corrections, producing the accurate transcription ‚ÄúAutopilot disengaged. Reduce altitude to 5,000 feet,‚Äù and reporting no low-confidence regions. This clearly demonstrated that the upgraded Whisper-large pipeline was far more reliable, context-aware, and suitable for aviation-grade speech recognition than the earlier Whisper-base + T5 approach

Conclusion:

In this project, I built an aircraft defect detection system using a balanced YOLO model that accurately identifies crack, dent, scratch, paint-off, and missing-head defects. Along with this, I added a speech recognition module to handle stressed pilot commands. My first approach used Whisper-base with a FLAN-T5 correction layer, but it often misheard critical phrases like ‚Äúautopilot disengaged,‚Äù making it unreliable. I then upgraded to Whisper-large with a confidence-based self-correction method, which accurately transcribed even noisy and distorted audio. This final setup made both the defect detection and speech components highly reliable and suitable for aviation conditions.
"""

